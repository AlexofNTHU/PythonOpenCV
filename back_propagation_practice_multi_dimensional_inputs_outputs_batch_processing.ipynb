{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "back_propagation_practice_multi-dimensional_inputs_outputs_batch_processing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "96xMUyR-_Q0q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This example is written for demonstrating a two-layer CNN giving multi-dimensional inputs and outputs in the sense of mini-batch.\n",
        "# Only a relu is put after the 1st layer.\n",
        "# 2019 Alex\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib\n",
        "#import Image\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# setting 1\n",
        "#x = np.random.randn(10, 10)\n",
        "# setting 2\n",
        "x = np.ones(100)\n",
        "x = x.reshape(10,10)\n",
        "\n",
        "\n",
        "# setting 1\n",
        "#y2_GT = 2*np.random.randn(10, 10)\n",
        "# setting 2\n",
        "y2_GT = 2*np.ones(100)\n",
        "y2_GT = y2_GT.reshape(10,10)\n",
        "\n",
        "print(\"input vector\")\n",
        "print(x)\n",
        "print(\"goal of learning=\" )\n",
        "print(y2_GT)\n",
        "\n",
        "# setting 1: Randomly initialize weights\n",
        "#w1 = np.random.randn(10, 10)\n",
        "#w2 = np.random.randn(10, 10)\n",
        "# setting 2\n",
        "w1 = np.ones(100)\n",
        "w1 = w1.reshape(10,10)\n",
        "w2 = np.ones(100)\n",
        "w2 = w2.reshape(10,10)\n",
        "\n",
        "\n",
        "# setting 1\n",
        "#learning_rate = 0.001 #0.01: gradient explosion, 0.001 is a good one\n",
        "#iterations=100 # 5000 would entirely fit the input\n",
        "# setting 2\n",
        "learning_rate = 0.00001 #0.0001 could observe dead relu is a good one; 0.00001 could work\n",
        "iterations=300 # 3000 would entirely fit the input; 100 is not enough.\n",
        "\n",
        "# for displaying the variation of weighting of 1st layer and 2nd layer\n",
        "w1_history= np.zeros(iterations)\n",
        "w2_history= np.zeros(iterations)\n",
        "# for displaying the output of the 1st layer\n",
        "y1_history= np.zeros(iterations)\n",
        "loss_history= np.zeros(iterations)\n",
        "\n",
        "\n",
        "for t in range(iterations):\n",
        "    # 1st layer inference        \n",
        "    y1 = x.dot(w1)\n",
        "    \n",
        "    # doing relu for the output of 1st layer\n",
        "    y1_relu = np.maximum(y1, 0) # result is a matrix\n",
        "\n",
        "    #print \"y1_relu=\"    \n",
        "    #print y1_relu\n",
        "\n",
        "    # performing 2nd layer computation\n",
        "    y2_pred = y1_relu.dot(w2) # result is a row vector\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y2_pred - y2_GT).sum()\n",
        "    #print y_pred.shape\n",
        "    #print np.mean(y_pred)\n",
        "    loss_history[t]=loss\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y2_pred = 2.0 * (y2_pred - y2_GT) # d_loss/d_y2        \n",
        "    grad_w2 = y1_relu.T.dot(grad_y2_pred)# (d_y2/d_w2)*(d_loss/d_y2)=d_loss/d_w2    \n",
        "    grad_y1_relu = grad_y2_pred.dot(w2.T) # (d_loss/d_y2)*(d_y2/d_y1)=d_loss/d_y1\n",
        "    grad_y1 = grad_y1_relu.copy()\n",
        "    grad_y1[y1 < 0] = 0 # only numbers through relu would be conducted backward pass\n",
        "    grad_w1 = x.T.dot(grad_y1) # (d_y1/d_w1)*(d_loss/d_y2)*(d_y2/d_y1)=(d_y1/d_w1)*(d_loss/d_y1)=d_loss/d_w1\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "            \n",
        "# \n",
        "\n",
        "print(\"y_expected\")\n",
        "print(y2_GT)\n",
        "\n",
        "print(\"y2_pred\")\n",
        "print(y2_pred)\n",
        "\n",
        "input_epoch = np.array(range(iterations))\n",
        "\n",
        "plt.plot(input_epoch,loss_history)\n",
        "plt.legend(['loss'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2w-Uh29x_dWT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}