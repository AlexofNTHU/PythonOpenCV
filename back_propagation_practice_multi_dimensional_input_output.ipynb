{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "back_propagation_practice_multi-dimensional_input_output.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TZOpLq0r-1NQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This example is written for demonstrating a two-layer CNN.\n",
        "# Both input and output are multi-dimensional vectors\n",
        "# Only a relu is put after the 1st layer.\n",
        "# 2019 Alex\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib\n",
        "\n",
        "\n",
        "# setting 1\n",
        "#x = np.random.randn(1, 10)\n",
        "# setting 2\n",
        "x = np.ones(10)\n",
        "x = x.reshape(1,10)\n",
        "\n",
        "\n",
        "# setting 1\n",
        "#y2_GT = 2*np.random.randn(1, 10)\n",
        "# setting 2\n",
        "y2_GT = 2*np.ones(10)\n",
        "y2_GT = y2_GT.reshape(1,10)\n",
        "\n",
        "print(\"input vector\")\n",
        "print(x)\n",
        "print(\"expected output vector=\")\n",
        "print(y2_GT)\n",
        "\n",
        "# Randomly initialize weights\n",
        "# setting 1\n",
        "#w1 = np.random.randn(10, 10)\n",
        "#w2 = np.random.randn(10, 10)\n",
        "\n",
        "# setting 2\n",
        "w1 = np.ones(100)\n",
        "w1 = w1.reshape(10,10)\n",
        "w2 = np.ones(100)\n",
        "w2 = w2.reshape(10,10)\n",
        "\n",
        "# setting 1\n",
        "#learning_rate = 0.001 #0.01 would result in dead relu; 0.001 would work\n",
        "#iterations=100  # 100 might not reach 0 loss but 1000 is more likely\n",
        "#\n",
        "\n",
        "# setting 2\n",
        "learning_rate = 0.0001 #0.001 would result in dead relu; 0.0001 would work\n",
        "iterations=500\n",
        "#\n",
        "\n",
        "# for displaying the variation of weighting of 1st layer and 2nd layer\n",
        "w1_history= np.zeros(iterations)\n",
        "w2_history= np.zeros(iterations)\n",
        "\n",
        "# for displaying the output of the 1st layer\n",
        "y1_history= np.zeros(iterations)\n",
        "Inference_result_history= np.zeros(iterations)\n",
        "loss_history= np.zeros(iterations)\n",
        "\n",
        "for t in range(iterations):\n",
        "    # 1st layer inference    \n",
        "    y1 = x.dot(w1)\n",
        "    \n",
        "    # doing relu for the output of 1st layer\n",
        "    y1_relu = np.maximum(y1, 0) # result is a row vector\n",
        "\n",
        "    #print \"y1_relu=\"\n",
        "    #print y1_relu\n",
        "\n",
        "    # store the output of the 1st layer\n",
        "    y1_history[t]= np.mean(y1_relu)\n",
        "\n",
        "    # performing 2nd layer computation\n",
        "    y2_pred = y1_relu.dot(w2) # result is a row vector\n",
        "\n",
        "    #print w2\n",
        "    #print y2_pred\n",
        "\n",
        "    Inference_result_history[t] = np.mean(y2_pred)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y2_pred - y2_GT).sum()\n",
        "    loss_history[t]=loss\n",
        "    #print y_pred.shape\n",
        "    #print np.mean(y_pred)\n",
        "\n",
        "    #print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y2_pred = 2.0 * (y2_pred - y2_GT) # d_loss/d_y2                 \n",
        "    grad_w2 = y1_relu.T.dot(grad_y2_pred)# (d_y2/d_w2)*(d_loss/d_y2)=d_loss/d_w2    \n",
        "    grad_y1_relu = grad_y2_pred.dot(w2.T) # (d_loss/d_y2)*(d_y2/d_y1)=d_loss/d_y1\n",
        "    grad_y1 = grad_y1_relu.copy()\n",
        "    grad_y1[y1 < 0] = 0 # only numbers through relu would be conducted backward pass\n",
        "    grad_w1 = x.T.dot(grad_y1) # (d_y1/d_w1)*(d_loss/d_y2)*(d_y2/d_y1)=(d_y1/d_w1)*(d_loss/d_y1)=d_loss/d_w1\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "\n",
        "    w1_history[t]= np.mean(w1)\n",
        "    w2_history[t]= np.mean(w2)\n",
        "\n",
        "    # forward result of each iteration\n",
        "\n",
        "# \n",
        "\n",
        "\n",
        "print(\"y2_pred\")\n",
        "print(y2_pred)\n",
        "\n",
        "input_epoch = np.array(range(iterations))\n",
        "\n",
        "plt.plot(input_epoch,loss_history)\n",
        "plt.legend(['loss'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nSHyuoRP_Gxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}