{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "back_propagation_practice.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "jv8TJiI_-FKP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This example is written for demonstrating a two-layer CNN.\n",
        "# Only a relu is put after the 1st layer.\n",
        "# 2019 Alex\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib\n",
        "\n",
        "# setting 1\n",
        "#x = np.random.randn(1, 1)\n",
        "# setting 2\n",
        "x = np.array(0.5)\n",
        "x.reshape(1,1) # not necessary actually\n",
        "\n",
        "# setting 1\n",
        "#y2_GT = np.random.randn(1, 1)\n",
        "# setting 2\n",
        "y2_GT=np.array(2.0)\n",
        "y2_GT.reshape(1,1)\n",
        "\n",
        "print(\"initial input=%f\"%x)\n",
        "print(\"goal of learning=%f\"%y2_GT)\n",
        "\n",
        "# Randomly initialize weights\n",
        "# setting 1\n",
        "#w1 = np.random.randn(1, 1)\n",
        "#w2 = np.random.randn(1, 1)\n",
        "# setting 2\n",
        "w1 = np.array(0.2)\n",
        "w1 = w1.reshape(1,1)\n",
        "w2 = np.array(0.5)\n",
        "w2 = w2.reshape(1,1)\n",
        "\n",
        "# setting 1\n",
        "#learning_rate = 0.001 #0.01 is a good one\n",
        "# setting 2\n",
        "learning_rate = 0.1 #0.01 is a good one\n",
        "\n",
        "iterations=100\n",
        "\n",
        "\n",
        "# for displaying the variation of weighting of 1st layer and 2nd layer\n",
        "w1_history= np.zeros(iterations)\n",
        "w2_history= np.zeros(iterations)\n",
        "# for displaying the output of the 1st layer\n",
        "y1_history= np.zeros(iterations)\n",
        "Inference_result_history= np.zeros(iterations)\n",
        "#forward_result=[]\n",
        "\n",
        "\n",
        "\n",
        "for t in range(iterations):\n",
        "    # 1st layer inference\n",
        "    y1 = x.dot(w1)\n",
        "    # doing relu for the output of 1st layer\n",
        "    y1_relu = np.maximum(y1, 0)\n",
        "\n",
        "    # store the output of the 1st layer\n",
        "    y1_history[t]= y1_relu    \n",
        "\n",
        "    # 2nd layer inference\n",
        "    y2_pred = y1_relu.dot(w2)\n",
        "\n",
        "    # output of the whole neural net\n",
        "    Inference_result_history[t]=y2_pred\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = np.square(y2_pred - y2_GT).sum()\n",
        "    #print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y2_pred = 2.0 * (y2_pred - y2_GT) # d_loss/d_y2    \n",
        "    grad_w2 = y1_relu.dot(grad_y2_pred) # (d_loss/d_y2)*(d_y2/d_w2)=d_loss/d_w2\n",
        "    grad_y1_relu = grad_y2_pred.dot(w2) # (d_loss/d_y2)*(d_y2/d_y1)=d_loss/d_y1\n",
        "    grad_y1 = grad_y1_relu.copy()\n",
        "    grad_y1[y1 < 0] = 0 # only weightings through relu would be conducted back pass\n",
        "    grad_w1 = x.dot(grad_y1) # (d_loss/d_y2)*(d_y2/d_y1)*(d_y1/d_w1)=(d_loss/d_y1)*(d_y1/d_w1)=d_loss/d_w1\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "\n",
        "    w1_history[t]= w1\n",
        "    w2_history[t]= w2\n",
        "\n",
        "    # forward result of each iteration\n",
        "\n",
        "# \n",
        "\n",
        "#print \"y2_pred\"\n",
        "#print y2_pred\n",
        "\n",
        "\n",
        "input_epoch = np.array(range(iterations))\n",
        "\n",
        "print(Inference_result_history)\n",
        "\n",
        "# for observing dead relu\n",
        "#print y1_history\n",
        "\n",
        "plt.plot(input_epoch,Inference_result_history,'*')\n",
        "plt.plot(input_epoch,w1_history,'+')\n",
        "plt.plot(input_epoch,w2_history,'-.')\n",
        "plt.plot(input_epoch,y1_history,'-')\n",
        "plt.legend(['result','w1','w2','y1'])\n",
        "plt.xlabel('iterations')\n",
        "plt.ylabel('values')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HsClU_WV-ZPY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}